training:
  batch_size: 128
  n_epochs: 500000
  n_iters: 300001
  snapshot_freq: 50000
  snapshot_sampling: true
  anneal_power: 2
  log_all_sigmas: false

sampling:
  batch_size: 10
  data_init: false
  step_lr: 0.0002
  n_steps_each: 5
  ckpt_id: 50000
  final_only: true
  fid: false
  denoise: true
  num_samples4fid: 10
  inpainting: false
  interpolation: false
  n_interpolations: 15
  linear_inverse: true
  likelihood_scale: 1

fast_fid:
  batch_size: 1000
  num_samples: 1000
  step_lr: 0.0000005
  n_steps_each: 5
  begin_ckpt: 3000
  end_ckpt: 3000
  verbose: false
  ensemble: true

test:
  begin_ckpt: 2
  end_ckpt: 5
  batch_size: 1

data:
  dataset: "MNIST"
  image_size: 28
  channels: 1
  logit_transform: false
  uniform_dequantization: false
  gaussian_dequantization: false
  random_flip: true
  rescaled: false
  num_workers: 4

measurements:
  measure_size: 200
  noise_variance: 0.0025
  quantization: true # set true when considering quantization
  quantize_bits: 1 # number of quantized bits Q

model:
  sigma_begin: 50
  num_classes: 232
  ema: true
  ema_rate: 0.999
  spec_norm: false
  sigma_dist: geometric
  sigma_end: 0.01
  normalization: InstanceNorm++
  nonlinearity: elu
  ngf: 128

optim:
  weight_decay: 0.000
  optimizer: "Adam"
  lr: 0.0001
  beta1: 0.9
  amsgrad: false
  eps: 0.00000001
